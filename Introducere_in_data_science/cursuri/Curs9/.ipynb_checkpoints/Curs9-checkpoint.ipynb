{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curs 9. Lucrul cu date de tip text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: prezentarea si codul sunt conform [Introduction to Machine Learning with Python: A Guide for Data Scientists](https://www.bookdepository.com/Introduction-Machine-Learning-with-Python-Andreas-C-Mueller-Sarah-Guido/9781449369415)\n",
    "\n",
    "O clasa aparte de probleme este cea data de procesarea de informatie de tip text. Exemple de probleme:\n",
    "* clasificarea mailului ca fiind de tip spam sau legitim\n",
    "* analiza sentimentelor pe baza elementelor scrise\n",
    "* regasirea de texte similare\n",
    "\n",
    "Exista urmatoarele 4 categorii de date text:\n",
    "1. date categoriale\n",
    "1. text liber reductibil la date categoriale\n",
    "1. text structurat\n",
    "1. text liber\n",
    "\n",
    "**Datele categoriale** provin dintr-o lista predefinita (ex: genul unei persoane, culoarea unei masini):\n",
    "* \"red\", \"green\", \"blue\", \"yellow\", \"black\", \"white\", \"purple\", \"pink\"; \"multicolo(u)red\", \"other\"\n",
    "* \"male\", \"female\", \"not declared\". \n",
    "\n",
    "Totusi, in functie de modul de prelaure a valorii de la utilizator, este inca posibil sa apara erori de scriere: blak in loc de black, sau particularitati de scriere: gray/grey, neighbor/neighbour. Se impune deci o determinare a formelor corecte, de exemplu prin [distanta Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance) sau [algoritmi de spell-checking](https://norvig.com/spell-correct.html) - atentie la dialectul ales. In alte cazuri este nevoie de reducerea dictionarului, pentru notiuni care sunt foarte similare sau identice: \n",
    "* \"culoarea ierbii\" -> \"verde\", \n",
    "* definirea unor categorii de ocupatii iar la final \"Altele\"\n",
    "* [Colours](http://www.thedoghousediaries.com/1406)\n",
    "\n",
    "In alte cazuri, exprimari libere (texte de dimensiuni medii) pot fi reduse la **categorii cunoscute**. Pentru simplificarea procesarii, se recomanda evitarea de introducere de text liber si sa se permita alegerea dintr-o multime predefinita de valori:\n",
    "\n",
    "<img src=\"./images/dropdown.gif\" alt=\"Dropdown\" width=\"200px\"/>\n",
    "\n",
    "**Textul structurat** poate fi exemplificat prin documente XML sau fisiere JSON, care respecta o anumita sintaxa si eventual o schema. De exemplu, datele de contact au o structura (adresa fizica, persoana de contact, telefon), interpretarea lor facandu-se cu cunostinte de domeniu. Procesarea lor este un subiect separat. \n",
    "\n",
    "Ultima categorie - **textul liber** - este data de documente de intindere mai mare: email, carti, tweets, scrisori de intentie etc. Procesarea lor intra in zona Natural Language Processing (NLP) si information retrieval (IR). Un exemplu este  determinarea plagiatelor, prin care se detecteaza preluari masive din alte surse text, sau atribuirea autorului - pentru o bucata de text se cere determinarea autorului cel mai probabil; [Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution](https://arxiv.org/abs/1609.06686); detectarea autorului unui cod compilat [Even Anonymous Coders Leave Fingerprints](https://www.wired.com/story/machine-learning-identify-anonymous-code/), articol la [When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries](https://arxiv.org/pdf/1512.08546.pdf).\n",
    "\n",
    "Sursele de date sunt diverse: \n",
    "* pagini Wikipedia\n",
    "* cartile din [proiectul Gutenberg](http://www.gutenberg.org/), la ora (aprilie 2021) peste 60000 de ebooks\n",
    "* [mesaje newgroups](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html)\n",
    "* [ziare](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html)\n",
    "* [Alphabetical list of free/public domain datasets with text data for use in Natural Language Processing (NLP)](https://github.com/niderhoff/nlp-datasets)\n",
    "* [IMDB Movie Review Sentiment Classification (stanford)](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "* [Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/)\n",
    "* [Resursa 1](https://stats.stackexchange.com/questions/18905/where-to-find-a-large-text-corpus), [Colectia 2](https://lionbridge.ai/datasets/the-best-25-datasets-for-natural-language-processing/), [Colectia 3](https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplu de aplicatie: analiza sentimentor din recenzii de filme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un set de recenzii de filme facute de catre utilizator, disponibil [aici](http://ai.stanford.edu/~amaas/data/sentiment/). Pentru fiecare recenzie exista o valoare numerica intre 1 si 10, reprezentand o indicatie: daca este un review pozitiv sau negativ. Setul de date este impartit in subset de antrenare si de testare, iar pentru fiecare subset se gasesc doua directoare, respectiv pentru aprecieri pozitive (rating > 5) si negative (rating <= 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:24:02.297326Z",
     "start_time": "2021-04-18T18:24:01.762928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is data\n",
      " Volume Serial Number is 503D-AB7C\n",
      "\n",
      " Directory of d:\\work\\school\\cursuri\\Introducere_In_Data_Science\\cursuri\\Curs9\\2021\\data\\aclImdb\n",
      "\n",
      "04/18/2021  03:18 PM    <DIR>          .\n",
      "04/18/2021  03:18 PM    <DIR>          ..\n",
      "04/12/2011  08:14 PM           845,980 imdb.vocab\n",
      "06/12/2011  01:54 AM           903,029 imdbEr.txt\n",
      "06/26/2011  03:18 AM             4,037 README\n",
      "04/18/2021  03:15 PM    <DIR>          test\n",
      "04/18/2021  03:18 PM    <DIR>          train\n",
      "               3 File(s)      1,753,046 bytes\n",
      "               4 Dir(s)  1,267,933,794,304 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"data/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:24:08.755474Z",
     "start_time": "2021-04-18T18:24:02.300320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder PATH listing for volume data\n",
      "Volume serial number is 503D-AB7C\n",
      "D:\\WORK\\SCHOOL\\CURSURI\\INTRODUCERE_IN_DATA_SCIENCE\\CURSURI\\CURS9\\2021\\DATA\\ACLIMDB\n",
      "+---test\n",
      "|   +---neg\n",
      "|   \\---pos\n",
      "\\---train\n",
      "    +---neg\n",
      "    +---pos\n",
      "    \\---unsup\n"
     ]
    }
   ],
   "source": [
    "!tree \"data/aclImdb\" /A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:24:08.766447Z",
     "start_time": "2021-04-18T18:24:08.760461Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# path = './data/aclImdb/'\n",
    "path = r'c:\\temp\\data\\aclImdb'\n",
    "path_train = os.path.join(path, 'train')\n",
    "path_test = os.path.join(path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:24:12.025223Z",
     "start_time": "2021-04-18T18:24:08.768438Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:24:57.714923Z",
     "start_time": "2021-04-18T18:24:12.026163Z"
    }
   },
   "outputs": [],
   "source": [
    "####### !!!16 mins running time!!!\n",
    "reviews_train = load_files(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:24:57.726894Z",
     "start_time": "2021-04-18T18:24:57.716918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are train texts organized:  <class 'list'>\n",
      "How many texts in train subset 75000\n",
      "A text:  b'I loved the first Little Mermaid. I know the songs, I love the characters and I love the story. I can\\'t say anything like that about The Little Mermaid II: Return to the Sea. It was terrible. Let\\'s start with the story. The plot was a reversed copy of the first movie. Same situations, except in reverse! Ariel wanted to live on land, her daughter Melody (creative name) wants to live in the sea. Ariel was tricked by Ursula, Melody is tricked by Ursula\\'s sister, Morgana. Ursula had a sister?? Not sure where that came from. Besides being a strange copy of the first movie, this movies plot seemed tired and was uninteresting compared to the first movie. Now the characters: 1. Ariel- What happened to her??!! No longer the spunky, headstrong teenager we all knew and loved from the first movie, she has now \"grown up\" and her personality went down the drain. Her singing voice wasn\\'t as strong either, due to either Jodi Benson being a lot older, or the songs being so terrible that her talent was wasted. 2. Prince Eric- While he didn\\'t have a lot of personality in the first movie, like all Disney princes, somehow his new voice and his very few lines made him even more robotic. To top it off, he just can\\'t seem to defend himself, and Ariel becomes the tough one of the two. 3. Sebastian- Say goodbye to the lovable crustacean from the first movie, because a whiny, aggravating little crab just took his place. He also had no good songs in this movie. You can almost forget the glory he earned from singing the incredible \"Under the Sea\" and \"Kiss the Girl\" from the first movie, and it is very sad. 4. Flounder- They destroyed him!! He is not cute anymore, his voice is terrible, and he has kids now?? Who\\'s the mother??? 5. Morgana- She appears to be Ursula\\'s sister out for revenge against her mother, who always picked Ursula over her. So she plans to get King Triton\\'s trident to become the new ruler of the sea. Sound familiar? Anyway, she\\'s a very clich\\xc3\\xa9 villain and falls short of Ursula\\'s greatness as a villain. She epically fails at witchcraft, she\\'s not very tough, and not threatening at all. 6.Melody- Ariel and Eric\\'s daughter. Ironic name because she, unlike Ariel, can\\'t sing. Her voice is annoying, her friends (a walrus and a penguin?? Really?!) are not funny or likable, and she\\'s exactly the same as Ariel, in reverse and not as likable. Skip this one. Don\\'t watch any Disney sequel except for Lion King 2. This movie butchered the classic that lives in all our pleasant memories. I will look back at this movie and just laugh.'\n",
      "Associated review: 2\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print('How are train texts organized: ', type(text_train))\n",
    "print('How many texts in train subset', len(text_train))\n",
    "print('A text: ', text_train[30000])\n",
    "print('Associated review:', y_train[50100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.493501Z",
     "start_time": "2021-04-18T18:24:57.729884Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_test = load_files(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.504463Z",
     "start_time": "2021-04-18T18:25:08.495487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are texts organized:  <class 'list'>\n",
      "How many texts in train subset 25000\n",
      "The first text:  b'A great gangster flick, with brilliant performances by well-known actors with great action scenes? Well, not this one.<br /><br />It\\'s rather amazing to see such a wide cast of well-known actors, that have many good movies in their filmographies in such a movie, without doubt this may be one of the worst they could possibly appear in.<br /><br />First of all, the plot is as you\\'d expect it from your average gangster biography, nothing new, nothing fancy in it. The way it is told makes the movie look a LOT longer than it is (when i thought the two hours should be almost over, i was quite surprised that only 45 minutes had passed).<br /><br />The action scenes look a lot like those from 80ies TV series - the A-Team, for example. It\\'s just that in the 80ies (esp. with the A-Team) those scenes were far more sophisticated than those in \"El Padrino\". It\\'s especially fun to see the guys point their guns in the air and still hit something (not to talk about people that take cover behind car doors which later look like they\\'ve been shot through).<br /><br />The acting fits quite nicely to the action. Either you get the same reaction to everything that happens (Dolph Lundgren style), or it\\'s so overacted that you may think it\\'s a parody (but unfortunately it\\'s not).<br /><br />My advise is to stay away from this movie, any other gangster movie is better than this one.'\n",
      "Associated review: 0\n"
     ]
    }
   ],
   "source": [
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print('How are texts organized: ', type(text_test))\n",
    "print('How many texts in train subset', len(text_test))\n",
    "print('The first text: ', text_test[70])\n",
    "print('Associated review:', y_test[70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se remarca existenta elementului html `<br />` ce poate fi inlaturat, fara a afecta continutul mesajului:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.811642Z",
     "start_time": "2021-04-18T18:25:08.507455Z"
    }
   },
   "outputs": [],
   "source": [
    "text_train = [text.replace(b'<br />', b' ') for text in text_train]\n",
    "text_test = [text.replace(b'<br />', b' ') for text in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numarul de elemente din fiecare clasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.822615Z",
     "start_time": "2021-04-18T18:25:08.813635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in train set:  [0 1 2]\n",
      "Classes in test set:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Classes in train set: ', np.unique(y_train))\n",
    "print('Classes in test set: ', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.936308Z",
     "start_time": "2021-04-18T18:25:08.824609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering out items of class \"2\" in train set\n",
    "text_train = [text_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]\n",
    "y_train = [y_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.954261Z",
     "start_time": "2021-04-18T18:25:08.938302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class in training set: [12500 12500]\n",
      "Samples per class in test set: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print('Samples per class in training set: {0}'.format(np.bincount(y_train)))\n",
    "print('Samples per class in test set: {0}'.format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentarea textului sub forma bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru un text, reprezenatrea bag of word se obtine astfel: se pleaca de la un vocabular  = multimea tuturor cuvintelor care pot aparea in texte - si se contorizeaza pentru fiecare cuvant din dictionar de cat ori apare in text. Daca vocabularul are $k$ cuvinte distincte, atunci pentru fiecare document rezulta un vector de $k$ componente. Majoritatea componentelor din vectorul asociat unui document va fi zero, deci avem de-a face cu vectori rari. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasii pentru obtinerea unei reprezentari bag of words sunt:\n",
    "1. despartirea in cuvinte (tokenizing)\n",
    "1. construirea dictionarului\n",
    "1. construirea documentului bag of word pentru document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bow_processing.png' width='600px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplu de obtinere de bag of words pe un text simplu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.962239Z",
     "start_time": "2021-04-18T18:25:08.956254Z"
    }
   },
   "outputs": [],
   "source": [
    "bards_words =[\n",
    "    \"The fool doth think he is wise,\",\n",
    "    \"but the wise man knows himself to be fool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasa `CountVectorizer` din `sklearn.feature_extraction.text` serveste la selectarea cuvintelor din text si calculul frecventei de aparitie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.973208Z",
     "start_time": "2021-04-18T18:25:08.965230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "print(vect.vocabulary_) # cuvintele distincte din texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.990163Z",
     "start_time": "2021-04-18T18:25:08.976200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 0), ('but', 1), ('doth', 2), ('fool', 3), ('he', 4), ('himself', 5), ('is', 6), ('knows', 7), ('man', 8), ('the', 9), ('think', 10), ('to', 11), ('wise', 12)]\n"
     ]
    }
   ],
   "source": [
    "# vocabular sortat in ordine alfabetica\n",
    "print(sorted(vect.vocabulary_.items(), key = lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtinerea unui vector bag of words se obtine prin aplicarea metodei `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:08.999139Z",
     "start_time": "2021-04-18T18:25:08.993156Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = vect.transform(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:09.006121Z",
     "start_time": "2021-04-18T18:25:09.001134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprezentarea ca vectori:\n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print('Reprezentarea ca vectori:\\n', bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:09.020083Z",
     "start_time": "2021-04-18T18:25:09.013104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprezentarea ca vectori rari\n",
      ":\b  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "print(f'Reprezentarea ca vectori rari\\n:\\b{bow}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformarea celor doua subseturi de date in BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:22.078631Z",
     "start_time": "2021-04-18T18:25:09.024073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(text_train)\n",
    "print(repr(X_train))\n",
    "X_test = vect.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:22.086575Z",
     "start_time": "2021-04-18T18:25:22.080590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular: 74849\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensiune vocabular: {len(vect.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:22.172378Z",
     "start_time": "2021-04-18T18:25:22.088568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiunea vocabularului: 74849\n"
     ]
    }
   ],
   "source": [
    "# obtinerea vocabularului\n",
    "feature_names = vect.get_feature_names()\n",
    "print('Dimensiunea vocabularului:', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:22.182318Z",
     "start_time": "2021-04-18T18:25:22.175338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:22.194286Z",
     "start_time": "2021-04-18T18:25:22.185311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['är', 'ääliöt', 'äänekoski', 'åge', 'åmål', 'æsthetic', 'écran', 'élan', 'émigré', 'émigrés', 'était', 'état', 'étc', 'évery', 'êxtase', 'ís', 'ísnt', 'østbye', 'über', 'üvegtigris']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:22.205256Z",
     "start_time": "2021-04-18T18:25:22.199274Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draper', 'draperies', 'drapery', 'drapes', 'draskovic', 'drastic', 'drastically', 'drat', 'dratch', 'dratic', 'dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[20000:20020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antrenarea si evaluarea unui model de logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:22.381196Z",
     "start_time": "2021-04-18T18:25:22.208250Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:35.833285Z",
     "start_time": "2021-04-18T18:25:22.383190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea medie de clasificare pe setul de instruire: 0.88272\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv = 5, n_jobs=4)\n",
    "print('Acuratetea medie de clasificare pe setul de instruire: {0}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicare de cross validation pentru parametrul de regularizare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un parametru de regularizare care determina penalizarea $L_2$ a ponderilor modelului. Efectuam cautarea valorii potrivite a acestui hiperparametru, al carui nume de argument este `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:35.851231Z",
     "start_time": "2021-04-18T18:25:35.842264Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:26:39.472519Z",
     "start_time": "2021-04-18T18:25:35.858216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.8881599999999998\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# param_grid = {'C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "param_grid = {'C':[0.0001, 0.01, 0.1]}\n",
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:26:39.517399Z",
     "start_time": "2021-04-18T18:26:39.474514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea pe setul de antrenare: 0.97484\n"
     ]
    }
   ],
   "source": [
    "print(f'Acuratetea pe setul de antrenare: {grid.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:26:39.552306Z",
     "start_time": "2021-04-18T18:26:39.519394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea pe setul de testare: 0.87884\n"
     ]
    }
   ],
   "source": [
    "print(f'Acuratetea pe setul de testare: {grid.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificarea parametrilor pentru obtinerea BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putem cere retinerea doar a acelor cuvinte care apar intr-un numar minim de documente, de ex 5. In acest fel speram ca se elimina cuvintele care nu sunt larg partajate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:26:50.768511Z",
     "start_time": "2021-04-18T18:26:39.554300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  27271\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5)\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:27:28.039718Z",
     "start_time": "2021-04-18T18:26:50.770505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.88744\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:27:28.070638Z",
     "start_time": "2021-04-18T18:27:28.041713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8778\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminare cuvinte neinformative (stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista cuvinte care nu poarta multa informatie: prepozitii, conjunctii, sau cuvinte care se repeta des in toate documentele. Exista doua posibilitati de eliminare a acestora:\n",
    "1. Se foloseste o lista predefinita de stopwords, specifica limbii respective\n",
    "1. Se estimeaza valoarea informationala a cuvintelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru limba engleza sunt definite stopwords in sklearn.feature_extraction.text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:27:28.077617Z",
     "start_time": "2021-04-18T18:27:28.071633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(len(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:27:28.085595Z",
     "start_time": "2021-04-18T18:27:28.079612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'whereas', 'it', 'everywhere', 'more', 'cannot', 'top', 'ie', 'behind', 'fifteen', 'seemed', 'between', 'which', 'below', 'another', 'latter', 'only', 'thereupon', 'now', 'out']\n"
     ]
    }
   ],
   "source": [
    "print(list(ENGLISH_STOP_WORDS)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:27:38.165701Z",
     "start_time": "2021-04-18T18:27:28.088588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  26966\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words='english')\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:27:51.460576Z",
     "start_time": "2021-04-18T18:27:38.167695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.8837200000000001\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:27:51.490499Z",
     "start_time": "2021-04-18T18:27:51.463569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87256\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculul valorii tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency - inverse document frequency da pondere unui cuvant care apare frecvent intr-un document, dar nu in multe documente - deci are putere descriptiva pentru documentul in care apare frecvent. Pentru un document $d$ si un cuvant $w$, valoarea tfidf se calculeaza ca:\n",
    "$$\n",
    "tfidf(w, d) = tf(w, d) \\cdot \\log\\left( \\frac{N+1}{N_w+1} \\right) + 1\n",
    "$$\n",
    "unde: $tf(w, d)$ este numarul de aparitii ale lui $w$ in document $d$, $N$ e numarul total de documente din corpus, $N_w$ este numarul de documente din corpus care contin cuvantul $w$. Pentru fiecare document $d$ se calculeaza un astfel de vector, care in final este normalizat in norma $L_2$ la 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:29:03.851435Z",
     "start_time": "2021-04-18T18:27:51.493490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.8886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm='l2'), \n",
    "                     LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "# param_grid = {'logisticregression__C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "param_grid = {'logisticregression__C':[0.01, 0.1, 1]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(text_train, y_train)\n",
    "print('Best cross validation score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:29:11.171853Z",
     "start_time": "2021-04-18T18:29:03.854427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8832"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizarea de n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW pierde succesiunea cuvintelor. Se pot considera toate succesiunile de n cuvinte - n-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:29:11.183820Z",
     "start_time": "2021-04-18T18:29:11.175842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The fool doth think he is wise,', 'but the wise man knows himself to be fool']\n"
     ]
    }
   ],
   "source": [
    "print(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:29:11.202769Z",
     "start_time": "2021-04-18T18:29:11.186812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be fool',\n",
       " 'but the',\n",
       " 'doth think',\n",
       " 'fool doth',\n",
       " 'he is',\n",
       " 'himself to',\n",
       " 'is wise',\n",
       " 'knows himself',\n",
       " 'man knows',\n",
       " 'the fool',\n",
       " 'the wise',\n",
       " 'think he',\n",
       " 'to be',\n",
       " 'wise man']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:29:11.226705Z",
     "start_time": "2021-04-18T18:29:11.208766Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "# param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:48:24.384574Z",
     "start_time": "2021-04-18T18:29:11.229698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidfvectorizer',\n",
       "                                        TfidfVectorizer(min_df=5)),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             n_jobs=4,\n",
       "             param_grid={'logisticregression__C': [0.01, 0.1, 1, 10],\n",
       "                         'tfidfvectorizer__ngram_range': [(1, 1), (1, 2),\n",
       "                                                          (1, 3)]})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:48:24.397540Z",
     "start_time": "2021-04-18T18:48:24.389561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.91\n",
      "Best parameters:\n",
      "{'logisticregression__C': 10, 'tfidfvectorizer__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:48:50.481831Z",
     "start_time": "2021-04-18T18:48:24.400531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99928"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:49:30.176973Z",
     "start_time": "2021-04-18T18:48:50.485821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90364"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alte modalitati de procesare a textelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2Vec: [King - Man + Woman = Queen: The Marvelous Mathematics of Computational Linguistics](https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/); [A Beginner's Guide to Word2Vec and Neural Word Embeddings](https://skymind.ai/wiki/word2vec); [Understanding Word Embeddings](https://hackernoon.com/understanding-word-embeddings-a9ff830403ce)\n",
    "* [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
